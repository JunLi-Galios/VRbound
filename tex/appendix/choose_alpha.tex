\section{Optimisation issues with $\alpha$-divergences and MC approximations}
\label{sec:opt}

It is in general an outstanding research question on how to select the divergence measure for a particular machine learning problem. In our case this corresponds to selecting the $\alpha$ value. Also an approximate inference algorithm can be evaluated with different performance measures, and it is impossible to find a single algorithm value that returns the best performance on all evaluations. Thus we only present the evaluation in test error and test log-likelihood in the main text. 

We discuss two conjectures to explain the difficulty of selecting $\alpha$ in the Bayesian neural network experiments. The first conjecture is that zero-forcing algorithms tend to favour minimising the test error, while mass-covering methods tend to improve the test log-likelihood. However zero-forcing methods can fail as it might miss an important mode due to local optima. Similarly mass-covering methods can be pathological if the exact posterior includes modes that are very far away from each other. Furthermore, the form of the posterior will change with the number of observed datapoints $N$, so the ``optimal'' setting of $\alpha$ for a fixed task may change with $N$. 

The second conjecture states that the MC approximation complicates the selection of $\alpha$, since it favours zero-forcing (because of the bias introduced). For example, in order to maximize the quantity of the MC approximation the algorithm need to make $\mathbb{E}[\hat{\mathcal{L}}_{\alpha, K}]$ finite first. However, Lemma \ref{lemma:alpha_k_non_exist} indicates that, if $\rho > 0$, then for finite sample size, there's a small probability $\rho^K$ that the MC approximation goes wrong. Hence to avoid this pathology the optimisation procedure will ensure $q = 0$ whenever $p$ is zero. Combining with Theorem 2, we conjecture that the MC approximation makes the algorithm more ``VI-like'' compared to the exact case. In other words, when MC approximation is deployed, the effective $\alpha$ value is closer to $\alpha = 1$ which is the value for VI (consider $K = 1$). This means, if there exists $\alpha_{\text{opt}} \neq 1$ for a specific task, in practice one should use $\alpha \leq \alpha_{\text{opt}}$ (for $\alpha_{\text{opt}} < 1$, and should use $\alpha \geq \alpha_{\text{opt}}$ if $\alpha_{\text{opt}} > 1$) when running the MC algorithm. In general one should be very careful when estimating the ratio between distribution with Monte Carlo methods. Also the introduced MC approach usually has higher variance compared to the variational case, so further control variate techniques should be applied to reduce the sampling variance.

Still we want to emphasize that for many problems, minimising an $\alpha$-divergence other than the KL-divergence can be very useful, even when with MC approximations. Approximate EP has been applied to deep Gaussian process regression and has shown to achieve the state-of-the-art results for benchmark datasets \cite{bui:dgp}. A recent paper \cite{depeweg:bnn_rl} tested BB-$\alpha$ for model-based reinforcement learning with Bayesian neural networks. In their tests using $\alpha = 0.5$ successfully captured the bi-modality and heteroskedasticity in the predictive distribution, while VI failed disastrously.